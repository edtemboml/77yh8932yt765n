{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>LLM - Using DistilBert for Protein Function Classification</h1>\n",
        "<h2> By Edwin Tembo - 2023</h2>\n",
        "*Not Fully Tested*"
      ],
      "metadata": {
        "id": "OgdCopXcb4se"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt1MyxKIzyMF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgnBQtf9vXiG"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTkCDy7NPuOx"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHxRRzqpBf76"
      },
      "outputs": [],
      "source": [
        "# Importing stock ml libraries\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import logging\n",
        "import ast\n",
        "import datetime\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "torch.cuda.empty_cache()\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "logger.addHandler(logging.StreamHandler(sys.stdout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxZOyiSMxwPl"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter(log_dir='/content/drive/MyDrive/protein/CAFA_TORCH_RUNS')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7N-SkxWC7zT"
      },
      "outputs": [],
      "source": [
        "# # Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr6XeiZW3YbT"
      },
      "outputs": [],
      "source": [
        "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    float( len(set_true.union(set_pred)) )\n",
        "        acc_list.append(tmp_a)\n",
        "    return np.mean(acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXQo4odYvDvM"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/protein/CAFA_SEQ_DATA/train_data.csv')\n",
        "data.drop(['id'], inplace=True, axis=1)\n",
        "data.rename(columns = {\"sequence\": \"text\"}, inplace=True)\n",
        "new_df = data\n",
        "new_df[\"labels\"] = new_df.labels.apply(ast.literal_eval)\n",
        "new_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpF4ZAaxC_OJ"
      },
      "outputs": [],
      "source": [
        "# Sections of config\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 1024\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "PRE_TRAINED_MODEL ='Rostlab/prot_bert'\n",
        "MODEL_VOCAB = '/content/drive/MyDrive/protein/CAFA_TORCH_MODELS/vocab_distilbert_protBert.bin'\n",
        "\n",
        "NUM_LABELS        = 500\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_VOCAB, truncation=True, do_lower_case=True)\n",
        "SEED = 567"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC7zA4nYDGX3"
      },
      "outputs": [],
      "source": [
        "class MultiLabelDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = self.data.labels\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        ##text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class InferenceDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOZWsLfGDOBG"
      },
      "outputs": [],
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 0.8\n",
        "train_data=new_df.sample(frac=train_size,random_state=SEED)\n",
        "test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
        "\n",
        "training_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
        "testing_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StbPlIyKDP9E"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeftvDhjDSPp"
      },
      "outputs": [],
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.\n",
        "\n",
        "class DistilBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL)\n",
        "        self.pre_classifier = torch.nn.Linear(MAX_LEN, MAX_LEN)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.lin1 = torch.nn.Linear(1024, 512)\n",
        "        self.lin2 = torch.nn.Linear(512, 256)\n",
        "        self.lin3 = torch.nn.Linear(256,64)\n",
        "        self.classifier = torch.nn.Linear(64, NUM_LABELS)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, num_pre_classifiers=1):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = self.lin1(pooler)\n",
        "        pooler = self.lin2(pooler)\n",
        "        pooler = self.lin3(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "model = DistilBERTClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ_wI0YwDVJZ"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO49FuR9DXsW"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvmZexewUhRE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def load_model_ckp(checkpoint_path, model, optimizer):\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  return model, optimizer, checkpoint['epoch'], checkpoint['loss']\n",
        "\n",
        "def save_model_ckp(epoch, model, optimizer,loss, save_path):\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, save_path)\n",
        "\n",
        "def save_model(model, model_dir, model_name='model.pth'):\n",
        "    path = os.path.join(model_dir, model_name)\n",
        "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
        "    torch.save(model.state_dict(), path)\n",
        "    logger.info(f\"Saving model: {path} \\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb9-Yr9YDZqo"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR = '/content/drive/MyDrive/5minit_prot/CAFA_TORCH_MODELS'\n",
        "CHECKPOINT_MODEL_PREFIX = 'distillBert_from_protbert'\n",
        "def train(epoch, model):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        losses.append(loss.item())\n",
        "        if _%5000==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "            now = datetime.datetime.now()\n",
        "            now = now.strftime(\"%Y%m%d%H%M%S\")\n",
        "            model_name = f\"{CHECKPOINT_MODEL_PREFIX}_epoch{epoch}_{now}.pth\"\n",
        "            save_path = os.path.join(CHECKPOINT_DIR,model_name)\n",
        "            save_model_ckp(epoch, model, optimizer,loss, save_path)\n",
        "\n",
        "            writer.add_scalar('Loss/Val', np.mean(losses), _)\n",
        "            ##writer.add_scalar('Accuracy/Val', acc, _)\n",
        "            writer.add_hparams(hparam_dict = {'lr': LEARNING_RATE, 'bsize': TRAIN_BATCH_SIZE} ,\n",
        "                       metric_dict = {'Loss/Val': np.mean(losses),\n",
        "                                      ##'Accuracy/Val' : acc,\n",
        "                                      'Step':_},\n",
        "                       hparam_domain_discrete=None,\n",
        "                       run_name=None)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Reta6H84DcJq"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp6SQ6iDlx0p"
      },
      "outputs": [],
      "source": [
        "# pool of size=3, stride=2\n",
        "m = torch.nn.MaxPool1d(20, stride=2)\n",
        "input = torch.randn(1024, 1)\n",
        "output = m(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bv3-E-bl5kw"
      },
      "outputs": [],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPqZKQ1BDfLW"
      },
      "outputs": [],
      "source": [
        "def validation(testing_loader, val_model):\n",
        "    val_model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "            ##fin_outputs.extend(torch.softmax(outputs,dim=1).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blZa10yR0yU4"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = '/content/drive/MyDrive/prot/CAFA_TORCH_MODELS'\n",
        "\n",
        "def model_fn(model_dir,\n",
        "             model_name,\n",
        "             num_classes=500):\n",
        "    logger.info('model_fn')\n",
        "    print('Loading the trained model...')\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = DistilBERTClass() # pass number of classes, in our case its 10\n",
        "    new_optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "    model, optimizer, epoch, loss = load_model_ckp(checkpoint_path=os.path.join(model_dir, model_name ),\n",
        "                    model=model,\n",
        "                   optimizer=new_optimizer)\n",
        "    return model.to(device)\n",
        "new_model = model_fn(model_dir=MODEL_DIR,\n",
        "                  model_name='distillBert_from_protbert_epoch0_20230608222539.pth',\n",
        "                  num_classes=500\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDdjmHR13_Nc"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch, new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yzLQmsdNItM"
      },
      "outputs": [],
      "source": [
        "# Saving the files for inference\n",
        "now = datetime.datetime.now()\n",
        "now = now.strftime(\"%Y%m%d%H%M%S\")\n",
        "output_dir = f'/content/drive/MyDrive/prot/{now}_CAFA_TORCH_MODELS'\n",
        "pathExists = os.path.exists(output_dir)\n",
        "if not pathExists:\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "WEIGHTS_NAME = f'{now}_distillBert_protBert.pth'\n",
        "CONFIG_NAME= f'{now}_distillBert_protBert.bin'\n",
        "\n",
        "model_to_save = new_model.module if hasattr(new_model, 'module') else new_model\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "##model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_vocabulary(output_dir)\n",
        "\n",
        "print('Saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQmJd7RvB4A-"
      },
      "outputs": [],
      "source": [
        "val_model_path = 'distillBert_from_protbert_epoch0_20230609164638.pth'\n",
        "\n",
        "val_model = model_fn(model_dir=MODEL_DIR,\n",
        "                  model_name=val_model_path,\n",
        "                  num_classes=500\n",
        "                   )\n",
        "outputs, targets = validation(testing_loader, val_model =val_model )\n",
        "\n",
        "final_outputs = np.array(outputs) >=0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9k50farZhQo"
      },
      "outputs": [],
      "source": [
        "final_outputs = np.array(outputs) >=0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qekc58XPDhIp"
      },
      "outputs": [],
      "source": [
        "val_hamming_loss = metrics.hamming_loss(targets, final_outputs)\n",
        "val_hamming_score = hamming_score(np.array(targets), np.array(final_outputs))\n",
        "\n",
        "print(f\"Hamming Score = {val_hamming_score}\")\n",
        "print(f\"Hamming Loss = {val_hamming_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUPPOn-JbFEu"
      },
      "outputs": [],
      "source": [
        "\n",
        "outputs, targets = validation(testing_loader, model =new_model )\n",
        "\n",
        "final_outputs = np.array(outputs) >=0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12ZPxu-Lik1P"
      },
      "outputs": [],
      "source": [
        "final_outputs = np.array(outputs) >=0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JijQyOKubON2"
      },
      "outputs": [],
      "source": [
        "val_hamming_loss = metrics.hamming_loss(targets, final_outputs)\n",
        "val_hamming_score = hamming_score(np.array(targets), np.array(final_outputs))\n",
        "val_f1_score      = metrics.f1_score(np.array(targets), np.array(final_outputs))\n",
        "print(f\"Hamming Score = {val_hamming_score}\")\n",
        "print(f\"Hamming Loss = {val_hamming_loss}\")\n",
        "print(f\"f1 Score = {val_f1_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quBnrPHXldI9"
      },
      "outputs": [],
      "source": [
        "submission_targets = pd.read_csv('/content/drive/MyDrive/prot/CAFA_SEQ_DATA/targets.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STqYujCflmcv"
      },
      "outputs": [],
      "source": [
        "submission_targets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl9WzKhLsDx2"
      },
      "outputs": [],
      "source": [
        "inf_model = model_fn(model_dir=MODEL_DIR,\n",
        "                  model_name='/content/drive/MyDrive/prot/CAFA_TORCH_MODELS/distillBert_from_protbert_epoch0_20230609164638.pth',\n",
        "                  num_classes=500\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dycpoizgmgv-"
      },
      "outputs": [],
      "source": [
        "\n",
        "submission_targets.drop(['id', 'sequence_length', 'taxonomyID'], inplace=True, axis=1)\n",
        "submission_targets.rename(columns = {\"sequence\": \"text\"}, inplace=True)\n",
        "submission_targets.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnRoabR7n0Nz"
      },
      "outputs": [],
      "source": [
        "targ_set = InferenceDataset(submission_targets, tokenizer, MAX_LEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITtDK2qYnmhQ"
      },
      "outputs": [],
      "source": [
        "targ_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "targ_loader = DataLoader(targ_set, **targ_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIkZJo5rovjI"
      },
      "outputs": [],
      "source": [
        "def doInference(targ_loader, targ_model):\n",
        "    targ_model.eval()\n",
        "\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(targ_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            fin_outputs.extend(torch.softmax(outputs,dim=1).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpjPuNEtpi1m"
      },
      "outputs": [],
      "source": [
        "inference_out = doInference(targ_loader = targ_loader, targ_model=inf_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQhQ8YlTw-d5"
      },
      "outputs": [],
      "source": [
        "inference_out= np.array(inference_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moaCpk5XxOfI"
      },
      "outputs": [],
      "source": [
        "inference_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG7M7n5VTuDw"
      },
      "outputs": [],
      "source": [
        "class_map = np.load('/content/drive/MyDrive/prot/CAFA_SEQ_DATA/class_map.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2be7SIlrVtye"
      },
      "outputs": [],
      "source": [
        "class_map = class_map.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GITgHM0Oxaiu"
      },
      "outputs": [],
      "source": [
        "submission_df = pd.DataFrame(inference_out, columns = class_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIVzrBrVTBB6"
      },
      "outputs": [],
      "source": [
        "submission_df[\"id\"]= submission_targets[\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV_1BRNFTX_e"
      },
      "outputs": [],
      "source": [
        "submission_df.set_index(\"id\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-iIAtF0WzYu"
      },
      "outputs": [],
      "source": [
        "submission_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ8bvei0W_w2"
      },
      "outputs": [],
      "source": [
        "df_melted = submission_df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4egX_HKXsby"
      },
      "outputs": [],
      "source": [
        "df_melted.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCpgSwNhXyGF"
      },
      "outputs": [],
      "source": [
        "df_melted = df_melted.melt([\"id\"])\n",
        "df_melted.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAvYbyCRYDlw"
      },
      "outputs": [],
      "source": [
        "df_melted['variable'] = df_melted['variable'].str.replace(\"_GO\",\"GO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXPZMr3hZoED"
      },
      "outputs": [],
      "source": [
        "df_melted.to_csv('/content/drive/MyDrive/prot/CAFA_SUBMISSION/submission_2.tsv', header=False, index=False, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb3cIMAimk1H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>ScaleFormer - Time Series Analysis Using Transformers</h1>\n",
        "<h2>By Edwin Tembo - 2023 </h2>\n",
        "Source: https://github.com/borealisai/scaleformer\n"
      ],
      "metadata": {
        "id": "BLAhNoWVGYsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huGz9WIOMRqE",
        "outputId": "e3432af8-452a-40d2-9202-165838b648fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/scaleformer': No such file or directory\n"
          ]
        }
      ],
      "source": [
        " !rm -r /content/scaleformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtvlenrcmFgQ",
        "outputId": "388db55a-4570-45b3-fa9d-1e4677fe2e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'scaleformer'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 80 (delta 28), reused 75 (delta 26), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (80/80), 3.40 MiB | 14.27 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/BorealisAI/scaleformer.git\n",
        "!cd scaleformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ls20bbSoOXU",
        "outputId": "1fa323f6-4eb1-4e81-b3bd-2e9f4d9de430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcudnn8 libcudnn8-dev libnccl-dev libnccl2\n",
            "The following packages will be upgraded:\n",
            "  base-files cuda-toolkit-config-common libasn1-8-heimdal libgnutls30\n",
            "  libgssapi3-heimdal libhcrypto4-heimdal libheimbase1-heimdal\n",
            "  libheimntlm0-heimdal libhx509-5-heimdal libkrb5-26-heimdal libpam-modules\n",
            "  libpam-modules-bin libpam-runtime libpam0g libroken18-heimdal libudev1\n",
            "  libwind0-heimdal linux-libc-dev openssl tar\n",
            "20 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 4,173 kB of archives.\n",
            "After this operation, 43.0 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 base-files amd64 11ubuntu5.7 [60.4 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  cuda-toolkit-config-common 12.1.55-1 [16.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 tar amd64 1.30+dfsg-7ubuntu0.20.04.3 [240 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam0g amd64 1.3.1-5ubuntu4.6 [55.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam-modules-bin amd64 1.3.1-5ubuntu4.6 [41.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam-modules amd64 1.3.1-5ubuntu4.6 [260 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam-runtime all 1.3.1-5ubuntu4.6 [37.3 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libudev1 amd64 245.4-4ubuntu3.21 [75.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgnutls30 amd64 3.6.13-2ubuntu1.8 [829 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssl amd64 1.1.1f-1ubuntu2.17 [622 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libroken18-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [42.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libasn1-8-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [181 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libheimbase1-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [30.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libhcrypto4-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [88.1 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwind0-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [47.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libhx509-5-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [107 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libkrb5-26-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [207 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libheimntlm0-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [15.1 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgssapi3-heimdal amd64 7.7.0+dfsg-1ubuntu1.4 [96.5 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 linux-libc-dev amd64 5.4.0-147.164 [1,120 kB]\n",
            "Fetched 4,173 kB in 5s (862 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 20.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../base-files_11ubuntu5.7_amd64.deb ...\n",
            "Unpacking base-files (11ubuntu5.7) over (11ubuntu5.6) ...\n",
            "Setting up base-files (11ubuntu5.7) ...\n",
            "Installing new version of config file /etc/issue ...\n",
            "Installing new version of config file /etc/issue.net ...\n",
            "Installing new version of config file /etc/lsb-release ...\n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../tar_1.30+dfsg-7ubuntu0.20.04.3_amd64.deb ...\n",
            "Unpacking tar (1.30+dfsg-7ubuntu0.20.04.3) over (1.30+dfsg-7ubuntu0.20.04.2) ...\n",
            "Setting up tar (1.30+dfsg-7ubuntu0.20.04.3) ...\n",
            "update-alternatives: warning: forcing reinstallation of alternative /usr/sbin/rmt-tar because link group rmt is broken\n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam0g_1.3.1-5ubuntu4.6_amd64.deb ...\n",
            "Unpacking libpam0g:amd64 (1.3.1-5ubuntu4.6) over (1.3.1-5ubuntu4.4) ...\n",
            "Setting up libpam0g:amd64 (1.3.1-5ubuntu4.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-modules-bin_1.3.1-5ubuntu4.6_amd64.deb ...\n",
            "Unpacking libpam-modules-bin (1.3.1-5ubuntu4.6) over (1.3.1-5ubuntu4.4) ...\n",
            "Setting up libpam-modules-bin (1.3.1-5ubuntu4.6) ...\n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-modules_1.3.1-5ubuntu4.6_amd64.deb ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Unpacking libpam-modules:amd64 (1.3.1-5ubuntu4.6) over (1.3.1-5ubuntu4.4) ...\n",
            "Setting up libpam-modules:amd64 (1.3.1-5ubuntu4.6) ...\n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-runtime_1.3.1-5ubuntu4.6_all.deb ...\n",
            "Unpacking libpam-runtime (1.3.1-5ubuntu4.6) over (1.3.1-5ubuntu4.4) ...\n",
            "Setting up libpam-runtime (1.3.1-5ubuntu4.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../libudev1_245.4-4ubuntu3.21_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (245.4-4ubuntu3.21) over (245.4-4ubuntu3.19) ...\n",
            "Setting up libudev1:amd64 (245.4-4ubuntu3.21) ...\n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../libgnutls30_3.6.13-2ubuntu1.8_amd64.deb ...\n",
            "Unpacking libgnutls30:amd64 (3.6.13-2ubuntu1.8) over (3.6.13-2ubuntu1.7) ...\n",
            "Setting up libgnutls30:amd64 (3.6.13-2ubuntu1.8) ...\n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../00-openssl_1.1.1f-1ubuntu2.17_amd64.deb ...\n",
            "Unpacking openssl (1.1.1f-1ubuntu2.17) over (1.1.1f-1ubuntu2.16) ...\n",
            "Preparing to unpack .../01-cuda-toolkit-config-common_12.1.55-1_all.deb ...\n",
            "Unpacking cuda-toolkit-config-common (12.1.55-1) over (12.0.146-1) ...\n",
            "Preparing to unpack .../02-libroken18-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libroken18-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../03-libasn1-8-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libasn1-8-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../04-libheimbase1-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libheimbase1-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../05-libhcrypto4-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libhcrypto4-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../06-libwind0-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libwind0-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../07-libhx509-5-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libhx509-5-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../08-libkrb5-26-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libkrb5-26-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../09-libheimntlm0-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libheimntlm0-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../10-libgssapi3-heimdal_7.7.0+dfsg-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libgssapi3-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) over (7.7.0+dfsg-1ubuntu1.3) ...\n",
            "Preparing to unpack .../11-linux-libc-dev_5.4.0-147.164_amd64.deb ...\n",
            "Unpacking linux-libc-dev:amd64 (5.4.0-147.164) over (5.4.0-137.154) ...\n",
            "Setting up cuda-toolkit-config-common (12.1.55-1) ...\n",
            "Setting up linux-libc-dev:amd64 (5.4.0-147.164) ...\n",
            "Setting up libroken18-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up openssl (1.1.1f-1ubuntu2.17) ...\n",
            "Setting up libheimbase1-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libasn1-8-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libhcrypto4-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libwind0-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libhx509-5-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libkrb5-26-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libheimntlm0-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Setting up libgssapi3-heimdal:amd64 (7.7.0+dfsg-1ubuntu1.4) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n"
          ]
        }
      ],
      "source": [
        "! sudo apt upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tsc4rcPmw8L",
        "outputId": "ba4ae6e0-ba71-43b9-b29a-8ed8ac58766e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.5.1\n",
            "  Downloading matplotlib-3.5.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpmath==1.2.1\n",
            "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.21.5\n",
            "  Downloading numpy-1.21.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.4.0\n",
            "  Downloading pandas-1.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting reformer_pytorch==1.4.4\n",
            "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
            "Collecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit_learn==1.1.1\n",
            "  Downloading scikit_learn-1.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools==57.5.0\n",
            "  Downloading setuptools-57.5.0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.10.2\n",
            "  Downloading torch-1.10.2-cp39-cp39-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_dct==0.1.5\n",
            "  Downloading torch_dct-0.1.5-py3-none-any.whl (4.8 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 11)) (1.11.1)\n",
            "Collecting performer-pytorch\n",
            "  Downloading performer_pytorch-1.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 1)) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 1)) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.0->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 4)) (2022.7.1)\n",
            "Collecting product-key-memory\n",
            "  Downloading product_key_memory-0.1.10.tar.gz (3.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting local-attention\n",
            "  Downloading local_attention-1.8.5-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests==2.25.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 6)) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests==2.25.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 6)) (1.26.15)\n",
            "Collecting idna<3,>=2.5\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests==2.25.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 6)) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit_learn==1.1.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit_learn==1.1.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 7)) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit_learn==1.1.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.10.2->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 9)) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->-r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt (line 1)) (1.16.0)\n",
            "Building wheels for collected packages: axial-positional-embedding, product-key-memory\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2901 sha256=c7c9838752dc6c62917fcf32600a36b32a44236c2da7d610b02da015371df06c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/e0/51/fec72c3ac576d0559b7b3a328ec5dcbac4120cca74be9e49fc\n",
            "  Building wheel for product-key-memory (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for product-key-memory: filename=product_key_memory-0.1.10-py3-none-any.whl size=3068 sha256=8950c1afe1facd289da0310867945ed3f745ae04caeafab0089df8f9620c67ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/32/7e/d987c5136182f35d70de6f817578df1540651a9d878833a2cb\n",
            "Successfully built axial-positional-embedding product-key-memory\n",
            "Installing collected packages: mpmath, torch, setuptools, numpy, idna, einops, torch_dct, requests, product-key-memory, pandas, matplotlib, local-attention, axial-positional-embedding, scikit_learn, reformer_pytorch, performer-pytorch\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.6.1\n",
            "    Uninstalling setuptools-67.6.1:\n",
            "      Successfully uninstalled setuptools-67.6.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "yfinance 0.2.18 requires requests>=2.26, but you have requests 2.25.1 which is incompatible.\n",
            "tweepy 4.13.0 requires requests<3,>=2.27.0, but you have requests 2.25.1 which is incompatible.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.10.2 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.21.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.5.3, but you have pandas 1.4.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests>=2.27.0, but you have requests 2.25.1 which is incompatible.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 57.5.0 which is incompatible.\n",
            "arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 57.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed axial-positional-embedding-0.2.1 einops-0.6.1 idna-2.10 local-attention-1.8.5 matplotlib-3.5.1 mpmath-1.2.1 numpy-1.21.5 pandas-1.4.0 performer-pytorch-1.1.4 product-key-memory-0.1.10 reformer_pytorch-1.4.4 requests-2.25.1 scikit_learn-1.1.1 setuptools-57.5.0 torch-1.10.2 torch_dct-0.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/drive/MyDrive/5minit_fin/ScaleFormer/src/scaleformer/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CDXHWN-QGVdw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7V9zaeaqm4n",
        "outputId": "3aa4f5e0-f653-47f4-d50a-bd0a825ef236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Args in experiment:\n",
            "Namespace(is_training=1, use_multi_scale=True, prob_forecasting=False, scales=[16, 8, 4, 2, 1], scale_factor=2, model='FEDformerMS', data='custom', root_path='./data/ETT/', data_path='/content/drive/MyDrive/5minit_fin/ScaleFormer/Datasets/misc/btc_prelimDailyRawVwap15m.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, modes1=64, mode_type=0, version='Wavelets', mode_select='low', modes=64, L=3, base='legendre', cross_activation='tanh', bucket_size=4, n_hashes=4, film_ours=True, ab=2, ratio=0.5, film_version=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=3, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='adaptive', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0')\n",
            "Use GPU: cuda:0\n",
            "base legendre\n",
            "base legendre\n",
            "base legendre\n",
            "corss fourier correlation used!\n",
            "corss fourier correlation used!\n",
            "corss fourier correlation used!\n",
            "corss fourier correlation used!\n",
            "enc_modes: 48, dec_modes: 64\n",
            "NUMBER OF PARAMETERS IN MODEL: FEDformerMS: 114352863\n",
            ">>>>>>>start training : /content/drive/MyDrive/5minit_fin/ScaleFormer/Datasets/misc/btc_prelimDailyRawVwap15m_FEDformerMS_96_adaptive>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 56065\n",
            "val 7942\n",
            "test 15978\n",
            "ADAPTIVE_LOSS:  tensor([[1.5005]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[1.]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 1 | loss: 1.0319369\n",
            "\tspeed: 2.7307s/iter; left time: 47571.8512s\n",
            "\titers: 200, epoch: 1 | loss: 0.9453329\n",
            "\tspeed: 2.7572s/iter; left time: 47756.7666s\n",
            "\titers: 300, epoch: 1 | loss: 0.9008170\n",
            "\tspeed: 2.7567s/iter; left time: 47473.8451s\n",
            "\titers: 400, epoch: 1 | loss: 0.8018087\n",
            "\tspeed: 2.7570s/iter; left time: 47202.7223s\n",
            "\titers: 500, epoch: 1 | loss: 0.7329913\n",
            "\tspeed: 2.7571s/iter; left time: 46927.9731s\n",
            "\titers: 600, epoch: 1 | loss: 0.6731912\n",
            "\tspeed: 2.7571s/iter; left time: 46652.4323s\n",
            "\titers: 700, epoch: 1 | loss: 0.6425838\n",
            "\tspeed: 2.7554s/iter; left time: 46348.4801s\n",
            "\titers: 800, epoch: 1 | loss: 0.5144764\n",
            "\tspeed: 2.7552s/iter; left time: 46070.0375s\n",
            "\titers: 900, epoch: 1 | loss: 0.4899766\n",
            "\tspeed: 2.7546s/iter; left time: 45784.7522s\n",
            "\titers: 1000, epoch: 1 | loss: 0.7576711\n",
            "\tspeed: 2.7546s/iter; left time: 45509.2157s\n",
            "\titers: 1100, epoch: 1 | loss: 0.6845445\n",
            "\tspeed: 2.7546s/iter; left time: 45233.9266s\n",
            "\titers: 1200, epoch: 1 | loss: 0.4487699\n",
            "\tspeed: 2.7545s/iter; left time: 44956.4393s\n",
            "\titers: 1300, epoch: 1 | loss: 0.7048980\n",
            "\tspeed: 2.7552s/iter; left time: 44691.8576s\n",
            "\titers: 1400, epoch: 1 | loss: 0.6274782\n",
            "\tspeed: 2.7544s/iter; left time: 44402.9969s\n",
            "\titers: 1500, epoch: 1 | loss: 0.5168436\n",
            "\tspeed: 2.7553s/iter; left time: 44142.1212s\n",
            "\titers: 1600, epoch: 1 | loss: 0.5903322\n",
            "\tspeed: 2.7579s/iter; left time: 43908.4609s\n",
            "\titers: 1700, epoch: 1 | loss: 0.2677998\n",
            "\tspeed: 2.7563s/iter; left time: 43606.6907s\n",
            "Epoch: 1 cost time: 4827.972006320953\n",
            "Epoch: 1, Steps: 1752 | Train Loss: 0.7024385 Vali Loss: 2.0450084 Test Loss: 2.0682921\n",
            "Validation loss decreased (inf --> 2.045008).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "ADAPTIVE_LOSS:  tensor([[1.8729]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[0.3828]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 2 | loss: 0.2875588\n",
            "\tspeed: 8.0883s/iter; left time: 126736.0230s\n",
            "\titers: 200, epoch: 2 | loss: 0.2983386\n",
            "\tspeed: 2.7572s/iter; left time: 42927.2854s\n",
            "\titers: 300, epoch: 2 | loss: 0.5650073\n",
            "\tspeed: 2.7582s/iter; left time: 42666.4372s\n",
            "\titers: 400, epoch: 2 | loss: 0.2495243\n",
            "\tspeed: 2.7576s/iter; left time: 42381.5723s\n",
            "\titers: 500, epoch: 2 | loss: 0.2854548\n",
            "\tspeed: 2.7577s/iter; left time: 42106.8793s\n",
            "\titers: 600, epoch: 2 | loss: 0.2021373\n",
            "\tspeed: 2.7576s/iter; left time: 41830.5134s\n",
            "\titers: 700, epoch: 2 | loss: 0.3245518\n",
            "\tspeed: 2.7570s/iter; left time: 41544.8796s\n",
            "\titers: 800, epoch: 2 | loss: 0.0896142\n",
            "\tspeed: 2.7567s/iter; left time: 41265.2595s\n",
            "\titers: 900, epoch: 2 | loss: 0.1270706\n",
            "\tspeed: 2.7567s/iter; left time: 40989.6674s\n",
            "\titers: 1000, epoch: 2 | loss: -0.0311858\n",
            "\tspeed: 2.7564s/iter; left time: 40709.2094s\n",
            "\titers: 1100, epoch: 2 | loss: 0.2741015\n",
            "\tspeed: 2.7567s/iter; left time: 40438.4211s\n",
            "\titers: 1200, epoch: 2 | loss: 0.1477787\n",
            "\tspeed: 2.7568s/iter; left time: 40163.1634s\n",
            "\titers: 1300, epoch: 2 | loss: -0.0930466\n",
            "\tspeed: 2.7561s/iter; left time: 39877.4250s\n",
            "\titers: 1400, epoch: 2 | loss: 0.2342081\n",
            "\tspeed: 2.7558s/iter; left time: 39597.6713s\n",
            "\titers: 1500, epoch: 2 | loss: -0.0535680\n",
            "\tspeed: 2.7565s/iter; left time: 39331.7970s\n",
            "\titers: 1600, epoch: 2 | loss: -0.0861014\n",
            "\tspeed: 2.7561s/iter; left time: 39051.3773s\n",
            "\titers: 1700, epoch: 2 | loss: -0.1864111\n",
            "\tspeed: 2.7575s/iter; left time: 38795.2440s\n",
            "Epoch: 2 cost time: 4830.926656484604\n",
            "Epoch: 2, Steps: 1752 | Train Loss: 0.2082234 Vali Loss: 2.1618071 Test Loss: 2.1419375\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "ADAPTIVE_LOSS:  tensor([[0.9466]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[0.1469]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 3 | loss: -0.0469063\n",
            "\tspeed: 8.0587s/iter; left time: 112153.4362s\n",
            "\titers: 200, epoch: 3 | loss: -0.2316833\n",
            "\tspeed: 2.7575s/iter; left time: 38100.9878s\n",
            "\titers: 300, epoch: 3 | loss: -0.1996210\n",
            "\tspeed: 2.7580s/iter; left time: 37832.0756s\n",
            "\titers: 400, epoch: 3 | loss: -0.1065343\n",
            "\tspeed: 2.7576s/iter; left time: 37550.8636s\n",
            "\titers: 500, epoch: 3 | loss: -0.1059892\n",
            "\tspeed: 2.7581s/iter; left time: 37280.8401s\n",
            "\titers: 600, epoch: 3 | loss: -0.1187354\n",
            "\tspeed: 2.7578s/iter; left time: 37001.5089s\n",
            "\titers: 700, epoch: 3 | loss: -0.2059861\n",
            "\tspeed: 2.7608s/iter; left time: 36766.0482s\n",
            "\titers: 800, epoch: 3 | loss: -0.1643564\n",
            "\tspeed: 2.7623s/iter; left time: 36509.2452s\n",
            "\titers: 900, epoch: 3 | loss: -0.3157289\n",
            "\tspeed: 2.7612s/iter; left time: 36218.2976s\n",
            "\titers: 1000, epoch: 3 | loss: -0.3133578\n",
            "\tspeed: 2.7606s/iter; left time: 35934.2935s\n",
            "\titers: 1100, epoch: 3 | loss: -0.2097752\n",
            "\tspeed: 2.7607s/iter; left time: 35660.1208s\n",
            "\titers: 1200, epoch: 3 | loss: -0.3151030\n",
            "\tspeed: 2.7616s/iter; left time: 35395.4838s\n",
            "\titers: 1300, epoch: 3 | loss: -0.4034093\n",
            "\tspeed: 2.7593s/iter; left time: 35090.2178s\n",
            "\titers: 1400, epoch: 3 | loss: -0.2110048\n",
            "\tspeed: 2.7589s/iter; left time: 34808.6432s\n",
            "\titers: 1500, epoch: 3 | loss: -0.1752689\n",
            "\tspeed: 2.7580s/iter; left time: 34521.4059s\n",
            "\titers: 1600, epoch: 3 | loss: -0.2375820\n",
            "\tspeed: 2.7591s/iter; left time: 34259.3089s\n",
            "\titers: 1700, epoch: 3 | loss: -0.1945904\n",
            "\tspeed: 2.7578s/iter; left time: 33968.3368s\n",
            "Epoch: 3 cost time: 4834.677117109299\n",
            "Epoch: 3, Steps: 1752 | Train Loss: -0.1803647 Vali Loss: 2.1994224 Test Loss: 2.1944108\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "ADAPTIVE_LOSS:  tensor([[0.3988]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[0.0766]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 4 | loss: -0.2204828\n",
            "\tspeed: 8.0642s/iter; left time: 98100.6800s\n",
            "\titers: 200, epoch: 4 | loss: -0.1444311\n",
            "\tspeed: 2.7599s/iter; left time: 33297.7247s\n",
            "\titers: 300, epoch: 4 | loss: -0.3881904\n",
            "\tspeed: 2.7614s/iter; left time: 33040.6666s\n",
            "\titers: 400, epoch: 4 | loss: -0.1672383\n",
            "\tspeed: 2.7618s/iter; left time: 32769.0388s\n",
            "\titers: 500, epoch: 4 | loss: -0.3091546\n",
            "\tspeed: 2.7614s/iter; left time: 32487.7989s\n",
            "\titers: 600, epoch: 4 | loss: -0.2322840\n",
            "\tspeed: 2.7616s/iter; left time: 32213.5853s\n",
            "\titers: 700, epoch: 4 | loss: -0.3072233\n",
            "\tspeed: 2.7608s/iter; left time: 31928.7308s\n",
            "\titers: 800, epoch: 4 | loss: -0.5492209\n",
            "\tspeed: 2.7592s/iter; left time: 31633.8481s\n",
            "\titers: 900, epoch: 4 | loss: -0.0617077\n",
            "\tspeed: 2.7602s/iter; left time: 31369.7123s\n",
            "\titers: 1000, epoch: 4 | loss: -0.3474589\n",
            "\tspeed: 2.7613s/iter; left time: 31106.4445s\n",
            "\titers: 1100, epoch: 4 | loss: -0.2467479\n",
            "\tspeed: 2.7606s/iter; left time: 30822.2068s\n",
            "\titers: 1200, epoch: 4 | loss: -0.3326059\n",
            "\tspeed: 2.7600s/iter; left time: 30539.1731s\n",
            "\titers: 1300, epoch: 4 | loss: -0.4134424\n",
            "\tspeed: 2.7605s/iter; left time: 30269.2491s\n",
            "\titers: 1400, epoch: 4 | loss: -0.4158576\n",
            "\tspeed: 2.7613s/iter; left time: 30001.3263s\n",
            "\titers: 1500, epoch: 4 | loss: -0.4234619\n",
            "\tspeed: 2.7593s/iter; left time: 29703.7899s\n",
            "\titers: 1600, epoch: 4 | loss: -0.1170465\n",
            "\tspeed: 2.7591s/iter; left time: 29425.6052s\n",
            "\titers: 1700, epoch: 4 | loss: -0.1774720\n",
            "\tspeed: 2.7594s/iter; left time: 29153.1302s\n",
            "Epoch: 4 cost time: 4836.924476623535\n",
            "Epoch: 4, Steps: 1752 | Train Loss: -0.3140959 Vali Loss: 2.2233083 Test Loss: 2.2089114\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : /content/drive/MyDrive/5minit_fin/ScaleFormer/Datasets/misc/btc_prelimDailyRawVwap15m_FEDformerMS_96_adaptive<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 15978\n",
            "test shape: (499, 32, 96, 7) (499, 32, 96, 7)\n",
            "test shape: (15968, 96, 7) (15968, 96, 7)\n",
            "mse:2.0682926177978516, mae:0.4663481116294861\n",
            "running time: 244.9789526462555\n",
            "Use GPU: cuda:0\n",
            "base legendre\n",
            "base legendre\n",
            "base legendre\n",
            "corss fourier correlation used!\n",
            "corss fourier correlation used!\n",
            "corss fourier correlation used!\n",
            "corss fourier correlation used!\n",
            "enc_modes: 48, dec_modes: 64\n",
            "NUMBER OF PARAMETERS IN MODEL: FEDformerMS: 114352863\n",
            ">>>>>>>start training : /content/drive/MyDrive/5minit_fin/ScaleFormer/Datasets/misc/btc_prelimDailyRawVwap15m_FEDformerMS_96_adaptive>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 56065\n",
            "val 7942\n",
            "test 15978\n",
            "ADAPTIVE_LOSS:  tensor([[1.5005]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[1.]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 1 | loss: 1.0140951\n",
            "\tspeed: 2.7412s/iter; left time: 47754.3904s\n",
            "\titers: 200, epoch: 1 | loss: 0.9482162\n",
            "\tspeed: 2.7566s/iter; left time: 47746.8273s\n",
            "\titers: 300, epoch: 1 | loss: 0.8422319\n",
            "\tspeed: 2.7552s/iter; left time: 47447.1040s\n",
            "\titers: 400, epoch: 1 | loss: 0.8255901\n",
            "\tspeed: 2.7554s/iter; left time: 47174.3701s\n",
            "\titers: 500, epoch: 1 | loss: 0.7306408\n",
            "\tspeed: 2.7556s/iter; left time: 46902.9256s\n",
            "\titers: 600, epoch: 1 | loss: 0.6970339\n",
            "\tspeed: 2.7520s/iter; left time: 46566.1266s\n",
            "\titers: 700, epoch: 1 | loss: 0.7061622\n",
            "\tspeed: 2.7513s/iter; left time: 46279.9598s\n",
            "\titers: 800, epoch: 1 | loss: 0.6321000\n",
            "\tspeed: 2.7552s/iter; left time: 46069.5854s\n",
            "\titers: 900, epoch: 1 | loss: 0.6621893\n",
            "\tspeed: 2.7541s/iter; left time: 45776.7111s\n",
            "\titers: 1000, epoch: 1 | loss: 0.5257574\n",
            "\tspeed: 2.7545s/iter; left time: 45507.5444s\n",
            "\titers: 1100, epoch: 1 | loss: 0.5682403\n",
            "\tspeed: 2.7545s/iter; left time: 45231.4233s\n",
            "\titers: 1200, epoch: 1 | loss: 0.6340992\n",
            "\tspeed: 2.7558s/iter; left time: 44976.6835s\n",
            "\titers: 1300, epoch: 1 | loss: 0.5279419\n",
            "\tspeed: 2.7553s/iter; left time: 44694.2145s\n",
            "\titers: 1400, epoch: 1 | loss: 0.4376170\n",
            "\tspeed: 2.7561s/iter; left time: 44431.3317s\n",
            "\titers: 1500, epoch: 1 | loss: 0.3242880\n",
            "\tspeed: 2.7550s/iter; left time: 44137.1951s\n",
            "\titers: 1600, epoch: 1 | loss: 0.2991738\n",
            "\tspeed: 2.7587s/iter; left time: 43921.8735s\n",
            "\titers: 1700, epoch: 1 | loss: 0.4544542\n",
            "\tspeed: 2.7561s/iter; left time: 43604.2782s\n",
            "Epoch: 1 cost time: 4827.739759922028\n",
            "Epoch: 1, Steps: 1752 | Train Loss: 0.6999015 Vali Loss: 2.0491290 Test Loss: 2.0739329\n",
            "Validation loss decreased (inf --> 2.049129).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "ADAPTIVE_LOSS:  tensor([[1.8323]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[0.3711]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 2 | loss: 0.4351848\n",
            "\tspeed: 8.0854s/iter; left time: 126690.1804s\n",
            "\titers: 200, epoch: 2 | loss: 0.2530649\n",
            "\tspeed: 2.7552s/iter; left time: 42896.3771s\n",
            "\titers: 300, epoch: 2 | loss: 0.1865152\n",
            "\tspeed: 2.7561s/iter; left time: 42634.2514s\n",
            "\titers: 400, epoch: 2 | loss: 0.1729638\n",
            "\tspeed: 2.7555s/iter; left time: 42349.3599s\n",
            "\titers: 500, epoch: 2 | loss: 0.1700720\n",
            "\tspeed: 2.7510s/iter; left time: 42005.5346s\n",
            "\titers: 600, epoch: 2 | loss: 0.2019215\n",
            "\tspeed: 2.7539s/iter; left time: 41774.4534s\n",
            "\titers: 700, epoch: 2 | loss: 0.1562427\n",
            "\tspeed: 2.7542s/iter; left time: 41502.5207s\n",
            "\titers: 800, epoch: 2 | loss: 0.1054324\n",
            "\tspeed: 2.7536s/iter; left time: 41218.4976s\n",
            "\titers: 900, epoch: 2 | loss: -0.0371033\n",
            "\tspeed: 2.7545s/iter; left time: 40956.4994s\n",
            "\titers: 1000, epoch: 2 | loss: 0.0862386\n",
            "\tspeed: 2.7549s/iter; left time: 40686.9159s\n",
            "\titers: 1100, epoch: 2 | loss: -0.0336625\n",
            "\tspeed: 2.7568s/iter; left time: 40440.1767s\n",
            "\titers: 1200, epoch: 2 | loss: 0.0345122\n",
            "\tspeed: 2.7572s/iter; left time: 40169.3080s\n",
            "\titers: 1300, epoch: 2 | loss: 0.1729573\n",
            "\tspeed: 2.7579s/iter; left time: 39903.3733s\n",
            "\titers: 1400, epoch: 2 | loss: -0.0270369\n",
            "\tspeed: 2.7574s/iter; left time: 39620.4534s\n",
            "\titers: 1500, epoch: 2 | loss: -0.0265681\n",
            "\tspeed: 2.7570s/iter; left time: 39340.3375s\n",
            "\titers: 1600, epoch: 2 | loss: 0.0340196\n",
            "\tspeed: 2.7580s/iter; left time: 39078.7592s\n",
            "\titers: 1700, epoch: 2 | loss: -0.0960769\n",
            "\tspeed: 2.7575s/iter; left time: 38794.6707s\n",
            "Epoch: 2 cost time: 4828.842258453369\n",
            "Epoch: 2, Steps: 1752 | Train Loss: 0.1842154 Vali Loss: 2.1401215 Test Loss: 2.1295137\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "ADAPTIVE_LOSS:  tensor([[0.8866]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[0.1399]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 3 | loss: -0.1641342\n",
            "\tspeed: 8.0623s/iter; left time: 112203.1399s\n",
            "\titers: 200, epoch: 3 | loss: 0.0050538\n",
            "\tspeed: 2.7553s/iter; left time: 38070.5560s\n",
            "\titers: 300, epoch: 3 | loss: -0.3101985\n",
            "\tspeed: 2.7579s/iter; left time: 37830.7380s\n",
            "\titers: 400, epoch: 3 | loss: -0.0911786\n",
            "\tspeed: 2.7590s/iter; left time: 37569.1530s\n",
            "\titers: 500, epoch: 3 | loss: -0.1860068\n",
            "\tspeed: 2.7573s/iter; left time: 37269.7650s\n",
            "\titers: 600, epoch: 3 | loss: -0.1274927\n",
            "\tspeed: 2.7582s/iter; left time: 37006.5563s\n",
            "\titers: 700, epoch: 3 | loss: -0.2354277\n",
            "\tspeed: 2.7581s/iter; left time: 36729.1773s\n",
            "\titers: 800, epoch: 3 | loss: -0.2458574\n",
            "\tspeed: 2.7583s/iter; left time: 36456.6470s\n",
            "\titers: 900, epoch: 3 | loss: -0.2927580\n",
            "\tspeed: 2.7582s/iter; left time: 36178.9184s\n",
            "\titers: 1000, epoch: 3 | loss: -0.1466908\n",
            "\tspeed: 2.7575s/iter; left time: 35894.5589s\n",
            "\titers: 1100, epoch: 3 | loss: -0.2672021\n",
            "\tspeed: 2.7580s/iter; left time: 35625.3285s\n",
            "\titers: 1200, epoch: 3 | loss: -0.2627200\n",
            "\tspeed: 2.7589s/iter; left time: 35360.4873s\n",
            "\titers: 1300, epoch: 3 | loss: -0.3615592\n",
            "\tspeed: 2.7586s/iter; left time: 35081.7520s\n",
            "\titers: 1400, epoch: 3 | loss: -0.3182254\n",
            "\tspeed: 2.7580s/iter; left time: 34798.2453s\n",
            "\titers: 1500, epoch: 3 | loss: -0.3613733\n",
            "\tspeed: 2.7576s/iter; left time: 34516.4707s\n",
            "\titers: 1600, epoch: 3 | loss: -0.1922133\n",
            "\tspeed: 2.7566s/iter; left time: 34229.3088s\n",
            "\titers: 1700, epoch: 3 | loss: -0.1922651\n",
            "\tspeed: 2.7554s/iter; left time: 33938.1967s\n",
            "Epoch: 3 cost time: 4832.069799900055\n",
            "Epoch: 3, Steps: 1752 | Train Loss: -0.1861423 Vali Loss: 2.2577484 Test Loss: 2.2055600\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "ADAPTIVE_LOSS:  tensor([[0.3719]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[0.0747]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 4 | loss: -0.2066373\n",
            "\tspeed: 8.0601s/iter; left time: 98051.2810s\n",
            "\titers: 200, epoch: 4 | loss: -0.3631751\n",
            "\tspeed: 2.7573s/iter; left time: 33266.3254s\n",
            "\titers: 300, epoch: 4 | loss: -0.3162003\n",
            "\tspeed: 2.7571s/iter; left time: 32988.1510s\n",
            "\titers: 400, epoch: 4 | loss: -0.1951731\n",
            "\tspeed: 2.7581s/iter; left time: 32725.1738s\n",
            "\titers: 500, epoch: 4 | loss: -0.3082846\n",
            "\tspeed: 2.7567s/iter; left time: 32432.0854s\n",
            "\titers: 600, epoch: 4 | loss: -0.2768292\n",
            "\tspeed: 2.7566s/iter; left time: 32155.4203s\n",
            "\titers: 700, epoch: 4 | loss: -0.1586501\n",
            "\tspeed: 2.7566s/iter; left time: 31880.5001s\n",
            "\titers: 800, epoch: 4 | loss: -0.3146116\n",
            "\tspeed: 2.7559s/iter; left time: 31595.8815s\n",
            "\titers: 900, epoch: 4 | loss: -0.1150509\n",
            "\tspeed: 2.7591s/iter; left time: 31357.7326s\n",
            "\titers: 1000, epoch: 4 | loss: -0.3594065\n",
            "\tspeed: 2.7570s/iter; left time: 31058.1386s\n",
            "\titers: 1100, epoch: 4 | loss: -0.3578579\n",
            "\tspeed: 2.7571s/iter; left time: 30782.8687s\n",
            "\titers: 1200, epoch: 4 | loss: -0.2750270\n",
            "\tspeed: 2.7583s/iter; left time: 30520.4213s\n",
            "\titers: 1300, epoch: 4 | loss: -0.2262557\n",
            "\tspeed: 2.7580s/iter; left time: 30241.2877s\n",
            "\titers: 1400, epoch: 4 | loss: -0.2696891\n",
            "\tspeed: 2.7582s/iter; left time: 29967.6573s\n",
            "\titers: 1500, epoch: 4 | loss: -0.3180284\n",
            "\tspeed: 2.7577s/iter; left time: 29686.5827s\n",
            "\titers: 1600, epoch: 4 | loss: -0.4708461\n",
            "\tspeed: 2.7571s/iter; left time: 29404.2852s\n",
            "\titers: 1700, epoch: 4 | loss: -0.3245945\n",
            "\tspeed: 2.7564s/iter; left time: 29121.2351s\n",
            "Epoch: 4 cost time: 4831.488782405853\n",
            "Epoch: 4, Steps: 1752 | Train Loss: -0.3084882 Vali Loss: 2.2356508 Test Loss: 2.1953585\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : /content/drive/MyDrive/5minit_fin/ScaleFormer/Datasets/misc/btc_prelimDailyRawVwap15m_FEDformerMS_96_adaptive<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 15978\n",
            "test shape: (499, 32, 96, 7) (499, 32, 96, 7)\n",
            "test shape: (15968, 96, 7) (15968, 96, 7)\n",
            "mse:2.07393479347229, mae:0.4405004680156708\n",
            "running time: 245.10068345069885\n",
            "Use GPU: cuda:0\n",
            "base legendre\n",
            "base legendre\n",
            "base legendre\n",
            "corss fourier correlation used!\n",
            "corss fourier correlation used!\n",
            "corss fourier correlation used!\n",
            "corss fourier correlation used!\n",
            "enc_modes: 48, dec_modes: 64\n",
            "NUMBER OF PARAMETERS IN MODEL: FEDformerMS: 114352863\n",
            ">>>>>>>start training : /content/drive/MyDrive/5minit_fin/ScaleFormer/Datasets/misc/btc_prelimDailyRawVwap15m_FEDformerMS_96_adaptive>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 56065\n",
            "val 7942\n",
            "test 15978\n",
            "ADAPTIVE_LOSS:  tensor([[1.5005]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[1.]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 1 | loss: 1.0365876\n",
            "\tspeed: 2.7411s/iter; left time: 47753.0019s\n",
            "\titers: 200, epoch: 1 | loss: 0.9492877\n",
            "\tspeed: 2.7576s/iter; left time: 47763.6278s\n",
            "\titers: 300, epoch: 1 | loss: 0.8642879\n",
            "\tspeed: 2.7568s/iter; left time: 47475.5943s\n",
            "\titers: 400, epoch: 1 | loss: 0.8005192\n",
            "\tspeed: 2.7562s/iter; left time: 47189.7467s\n",
            "\titers: 500, epoch: 1 | loss: 0.7259895\n",
            "\tspeed: 2.7571s/iter; left time: 46929.2113s\n",
            "\titers: 600, epoch: 1 | loss: 0.7628557\n",
            "\tspeed: 2.7582s/iter; left time: 46671.6275s\n",
            "\titers: 700, epoch: 1 | loss: 0.6220240\n",
            "\tspeed: 2.7581s/iter; left time: 46394.5978s\n",
            "\titers: 800, epoch: 1 | loss: 0.5320107\n",
            "\tspeed: 2.7578s/iter; left time: 46113.7570s\n",
            "\titers: 900, epoch: 1 | loss: 0.7751915\n",
            "\tspeed: 2.7579s/iter; left time: 45839.2786s\n",
            "\titers: 1000, epoch: 1 | loss: 0.6063091\n",
            "\tspeed: 2.7579s/iter; left time: 45562.7652s\n",
            "\titers: 1100, epoch: 1 | loss: 0.5079337\n",
            "\tspeed: 2.7624s/iter; left time: 45360.7710s\n",
            "\titers: 1200, epoch: 1 | loss: 0.6749760\n",
            "\tspeed: 2.7575s/iter; left time: 45005.7465s\n",
            "\titers: 1300, epoch: 1 | loss: 0.4887133\n",
            "\tspeed: 2.7589s/iter; left time: 44751.6596s\n",
            "\titers: 1400, epoch: 1 | loss: 0.4542121\n",
            "\tspeed: 2.7583s/iter; left time: 44466.6197s\n",
            "\titers: 1500, epoch: 1 | loss: 0.4499716\n",
            "\tspeed: 2.7587s/iter; left time: 44197.5030s\n",
            "\titers: 1600, epoch: 1 | loss: 0.3604121\n",
            "\tspeed: 2.7571s/iter; left time: 43896.0971s\n",
            "\titers: 1700, epoch: 1 | loss: 0.2609063\n",
            "\tspeed: 2.7589s/iter; left time: 43649.2325s\n",
            "Epoch: 1 cost time: 4832.749681711197\n",
            "Epoch: 1, Steps: 1752 | Train Loss: 0.7015613 Vali Loss: 2.0673459 Test Loss: 2.0852449\n",
            "Validation loss decreased (inf --> 2.067346).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "ADAPTIVE_LOSS:  tensor([[1.8465]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[0.3746]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 2 | loss: 0.2754810\n",
            "\tspeed: 8.0930s/iter; left time: 126808.5854s\n",
            "\titers: 200, epoch: 2 | loss: 0.2465679\n",
            "\tspeed: 2.7566s/iter; left time: 42917.5570s\n",
            "\titers: 300, epoch: 2 | loss: 0.2764148\n",
            "\tspeed: 2.7565s/iter; left time: 42640.0398s\n",
            "\titers: 400, epoch: 2 | loss: 0.2361526\n",
            "\tspeed: 2.7558s/iter; left time: 42354.3352s\n",
            "\titers: 500, epoch: 2 | loss: 0.3576951\n",
            "\tspeed: 2.7560s/iter; left time: 42081.6039s\n",
            "\titers: 600, epoch: 2 | loss: 0.3530693\n",
            "\tspeed: 2.7560s/iter; left time: 41805.1441s\n",
            "\titers: 700, epoch: 2 | loss: 0.2455426\n",
            "\tspeed: 2.7551s/iter; left time: 41516.8050s\n",
            "\titers: 800, epoch: 2 | loss: 0.1588896\n",
            "\tspeed: 2.7567s/iter; left time: 41265.5059s\n",
            "\titers: 900, epoch: 2 | loss: 0.1372256\n",
            "\tspeed: 2.7584s/iter; left time: 41014.6680s\n",
            "\titers: 1000, epoch: 2 | loss: 0.0497362\n",
            "\tspeed: 2.7597s/iter; left time: 40757.8771s\n",
            "\titers: 1100, epoch: 2 | loss: 0.0115410\n",
            "\tspeed: 2.7618s/iter; left time: 40512.5356s\n",
            "\titers: 1200, epoch: 2 | loss: 0.1496589\n",
            "\tspeed: 2.7592s/iter; left time: 40198.1227s\n",
            "\titers: 1300, epoch: 2 | loss: 0.0539618\n",
            "\tspeed: 2.7585s/iter; left time: 39913.2321s\n",
            "\titers: 1400, epoch: 2 | loss: -0.0706913\n",
            "\tspeed: 2.7580s/iter; left time: 39629.8761s\n",
            "\titers: 1500, epoch: 2 | loss: -0.0536038\n",
            "\tspeed: 2.7588s/iter; left time: 39365.3380s\n",
            "\titers: 1600, epoch: 2 | loss: 0.0570451\n",
            "\tspeed: 2.7574s/iter; left time: 39069.6979s\n",
            "\titers: 1700, epoch: 2 | loss: -0.0860136\n",
            "\tspeed: 2.7608s/iter; left time: 38841.1406s\n",
            "Epoch: 2 cost time: 4832.70719575882\n",
            "Epoch: 2, Steps: 1752 | Train Loss: 0.1900417 Vali Loss: 2.1694257 Test Loss: 2.1575994\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "ADAPTIVE_LOSS:  tensor([[0.9010]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[0.1405]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 3 | loss: 0.0575131\n",
            "\tspeed: 8.0615s/iter; left time: 112192.3610s\n",
            "\titers: 200, epoch: 3 | loss: -0.1128686\n",
            "\tspeed: 2.7586s/iter; left time: 38115.2872s\n",
            "\titers: 300, epoch: 3 | loss: -0.2815009\n",
            "\tspeed: 2.7579s/iter; left time: 37830.3577s\n",
            "\titers: 400, epoch: 3 | loss: -0.1224484\n",
            "\tspeed: 2.7590s/iter; left time: 37569.0303s\n",
            "\titers: 500, epoch: 3 | loss: -0.2578575\n",
            "\tspeed: 2.7622s/iter; left time: 37337.2706s\n",
            "\titers: 600, epoch: 3 | loss: -0.2727382\n",
            "\tspeed: 2.7627s/iter; left time: 37067.7902s\n",
            "\titers: 700, epoch: 3 | loss: -0.1134593\n",
            "\tspeed: 2.7629s/iter; left time: 36793.8557s\n",
            "\titers: 800, epoch: 3 | loss: -0.2407410\n",
            "\tspeed: 2.7625s/iter; left time: 36512.5994s\n",
            "\titers: 900, epoch: 3 | loss: -0.2590110\n",
            "\tspeed: 2.7625s/iter; left time: 36235.5968s\n",
            "\titers: 1000, epoch: 3 | loss: -0.3425798\n",
            "\tspeed: 2.7634s/iter; left time: 35971.3300s\n",
            "\titers: 1100, epoch: 3 | loss: -0.1397335\n",
            "\tspeed: 2.7601s/iter; left time: 35651.8557s\n",
            "\titers: 1200, epoch: 3 | loss: -0.1087510\n",
            "\tspeed: 2.7583s/iter; left time: 35353.6704s\n",
            "\titers: 1300, epoch: 3 | loss: -0.2898837\n",
            "\tspeed: 2.7628s/iter; left time: 35134.4462s\n",
            "\titers: 1400, epoch: 3 | loss: -0.3257988\n",
            "\tspeed: 2.7670s/iter; left time: 34910.7615s\n",
            "\titers: 1500, epoch: 3 | loss: -0.2996852\n",
            "\tspeed: 2.7621s/iter; left time: 34573.1077s\n",
            "\titers: 1600, epoch: 3 | loss: -0.3937775\n",
            "\tspeed: 2.7622s/iter; left time: 34297.9076s\n",
            "\titers: 1700, epoch: 3 | loss: -0.2126923\n",
            "\tspeed: 2.7628s/iter; left time: 34029.4445s\n",
            "Epoch: 3 cost time: 4838.797844409943\n",
            "Epoch: 3, Steps: 1752 | Train Loss: -0.1864070 Vali Loss: 2.2164068 Test Loss: 2.1919057\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "ADAPTIVE_LOSS:  tensor([[0.3785]], device='cuda:0', grad_fn=<AddBackward0>) tensor([[0.0754]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\titers: 100, epoch: 4 | loss: -0.3970394\n",
            "\tspeed: 8.0713s/iter; left time: 98187.1707s\n",
            "\titers: 200, epoch: 4 | loss: -0.4821975\n",
            "\tspeed: 2.7600s/iter; left time: 33299.6611s\n",
            "\titers: 300, epoch: 4 | loss: -0.2464558\n",
            "\tspeed: 2.7595s/iter; left time: 33017.2555s\n",
            "\titers: 400, epoch: 4 | loss: -0.1903456\n",
            "\tspeed: 2.7598s/iter; left time: 32744.9731s\n",
            "\titers: 500, epoch: 4 | loss: -0.4080292\n",
            "\tspeed: 2.7592s/iter; left time: 32461.5653s\n",
            "\titers: 600, epoch: 4 | loss: -0.2117454\n",
            "\tspeed: 2.7587s/iter; left time: 32180.5939s\n",
            "\titers: 700, epoch: 4 | loss: -0.3984029\n",
            "\tspeed: 2.7602s/iter; left time: 31921.5415s\n",
            "\titers: 800, epoch: 4 | loss: -0.2324475\n",
            "\tspeed: 2.7597s/iter; left time: 31640.1335s\n",
            "\titers: 900, epoch: 4 | loss: -0.3471636\n",
            "\tspeed: 2.7592s/iter; left time: 31357.8016s\n",
            "\titers: 1000, epoch: 4 | loss: -0.0874288\n",
            "\tspeed: 2.7586s/iter; left time: 31076.0721s\n",
            "\titers: 1100, epoch: 4 | loss: -0.3917778\n",
            "\tspeed: 2.7597s/iter; left time: 30812.4964s\n",
            "\titers: 1200, epoch: 4 | loss: -0.1202987\n",
            "\tspeed: 2.7587s/iter; left time: 30525.2941s\n",
            "\titers: 1300, epoch: 4 | loss: -0.4883654\n",
            "\tspeed: 2.7594s/iter; left time: 30257.0838s\n",
            "\titers: 1400, epoch: 4 | loss: -0.3149356\n",
            "\tspeed: 2.7593s/iter; left time: 29980.2535s\n",
            "\titers: 1500, epoch: 4 | loss: -0.4196768\n",
            "\tspeed: 2.7591s/iter; left time: 29702.2433s\n",
            "\titers: 1600, epoch: 4 | loss: -0.3496568\n",
            "\tspeed: 2.7564s/iter; left time: 29396.6910s\n",
            "\titers: 1700, epoch: 4 | loss: -0.3599863\n",
            "\tspeed: 2.7578s/iter; left time: 29136.5115s\n",
            "Epoch: 4 cost time: 4834.61333155632\n",
            "Epoch: 4, Steps: 1752 | Train Loss: -0.3159622 Vali Loss: 2.2250450 Test Loss: 2.2048335\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : /content/drive/MyDrive/5minit_fin/ScaleFormer/Datasets/misc/btc_prelimDailyRawVwap15m_FEDformerMS_96_adaptive<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 15978\n",
            "test shape: (499, 32, 96, 7) (499, 32, 96, 7)\n",
            "test shape: (15968, 96, 7) (15968, 96, 7)\n",
            "mse:2.0852437019348145, mae:0.4421622157096863\n",
            "running time: 245.14375686645508\n"
          ]
        }
      ],
      "source": [
        "!python -u /content/scaleformer/run.py --data_path '/content/drive/MyDrive/fin/ScaleFormer/Datasets/misc/btc_prelimDailyRawVwap15m.csv' --model FEDformerMS --pred_len 96 --loss adaptive  --enc_in 7  --dec_in 7 --c_out 7"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}